{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9FKzHsdt53U"
   },
   "source": [
    "# Bony Anatomy Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pd3G4QOJs0KE"
   },
   "outputs": [],
   "source": [
    "# OCI libraries\n",
    "import ads\n",
    "import ocifs\n",
    "from ocifs import OCIFileSystem\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import copy\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from io import BytesIO\n",
    "from gzip import GzipFile\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import pydicom\n",
    "import nibabel\n",
    "from nibabel import FileHolder, Nifti1Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import utils as smp_utils\n",
    "\n",
    "from torchmetrics.classification import MulticlassJaccardIndex, Dice\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hqi8DEP2sI34"
   },
   "outputs": [],
   "source": [
    "BONY_ANATOMY = \"knee\" # change to \"hip\" to run on hip data\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rjWsBFbt2Vh"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"OCI_RESOURCE_PRINCIPAL_VERSION\" in os.environ:\n",
    "    # Use resource principal\n",
    "    print(\"using Resource Principal for auth\")\n",
    "    ads.set_auth(auth=\"resource_principal\")\n",
    "else:\n",
    "    # Use api_key with config file\n",
    "    print(\"using API key for auth\")\n",
    "    ads.set_auth(auth=\"api_key\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = OCIFileSystem(region=\"us-ashburn-1\")\n",
    "bucket_name = \"Datasets\"\n",
    "namespace = \"idt35vaelvct\"    \n",
    "filename = \"JHIR_Hip_Knee_Datasets.zip\"\n",
    "\n",
    "data_root = f\"oci://{bucket_name}@{namespace}/\"\n",
    "\n",
    "# Creating the local directory \n",
    "dirpath = f\"./data/\"\n",
    "if not os.path.exists(dirpath):\n",
    "    os.makedirs(dirpath)\n",
    "\n",
    "# Downloading the data from Object Storage using OCIFS (https://github.com/oracle/ocifs)\n",
    "if os.path.exists(os.path.join(dirpath, filename)):\n",
    "    if not os.path.exists(os.path.join(dirpath, filename)):\n",
    "        with ZipFile(os.path.join(dirpath, filename), 'r') as zipf:\n",
    "            zipf.extractall(dirpath)\n",
    "else:\n",
    "    fs.download(f\"{data_root}{filename}\" ,os.path.join(dirpath, filename))\n",
    "    with ZipFile(os.path.join(dirpath, filename), 'r') as zipf:\n",
    "        zipf.extractall(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc8qVtmMsRB0"
   },
   "outputs": [],
   "source": [
    "# Directories for\n",
    "if BONY_ANATOMY == \"knee\":\n",
    "    csv_filename = \"data/JHIR_Hip_Knee_Datasets/Knee/segmentation.csv\"\n",
    "    imaging_data_root = \"data/JHIR_Hip_Knee_Datasets/Knee/\"\n",
    "    classes = {\n",
    "        0: \"Background\",\n",
    "        1: \"R Patella\",\n",
    "        2: \"R Femur\",\n",
    "        3: \"R Tibia\",\n",
    "        4: \"R Fibula\",\n",
    "        5: \"L Patella\",\n",
    "        6: \"L Femur\",\n",
    "        7: \"L Tibia\",\n",
    "        8: \"L Fibula\"\n",
    "    }\n",
    "\n",
    "else:\n",
    "    csv_filename = \"data/JHIR_Hip_Knee_Datasets/Hip/segmentation.csv\"\n",
    "    imaging_data_root = \"data/JHIR_Hip_Knee_Datasets/Hip/\"\n",
    "\n",
    "    classes = {\n",
    "        0: \"Background\",\n",
    "        1: \"R Acetabulum\",\n",
    "        2: \"L Acetabulum\",\n",
    "        3: \"R Ilium, Ischium, and Pubis\",\n",
    "        4: \"L Ilium, Ischium, and Pubis\",\n",
    "        5: \"R Femur\",\n",
    "        6: \"L Femur\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9YCLP7atBq9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2Qwhgqysxrw"
   },
   "outputs": [],
   "source": [
    "def generate_train_test_split(csv_pth, filter_query=None):\n",
    "    data_records = pd.read_csv(csv_pth)\n",
    "    data_records = data_records[data_records.id != 9025994].reset_index(drop=True)\n",
    "    if filter_query:\n",
    "        data_records = data_records.query(filter_query)\n",
    "\n",
    "    train, test = train_test_split(data_records.id.unique(), test_size=0.3, random_state=42)\n",
    "    valid, test = train_test_split(test, test_size=0.5, random_state=42)\n",
    "\n",
    "    train = data_records[data_records.id.isin(train)].reset_index(drop=True)\n",
    "    valid = data_records[data_records.id.isin(valid)].reset_index(drop=True)\n",
    "    test = data_records[data_records.id.isin(test)].reset_index(drop=True)\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eagrBr6TtHxt"
   },
   "outputs": [],
   "source": [
    "def balance_dataset(data, by_filter1, by_filter2):\n",
    "    filtered1 = data.query(by_filter1)\n",
    "    filtered2 = data.query(by_filter2)\n",
    "\n",
    "    min_sample_size = np.minimum(len(filtered1), len(filtered2))\n",
    "    samp1 = filtered1.sample(min_sample_size,  random_state = 42)\n",
    "    samp2 = filtered2.sample(min_sample_size,  random_state = 42)\n",
    "\n",
    "    balanced_data = pd.concat([samp1, samp2]).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Training dataset reduced from size of {len(data)} samples to a balanced dataset of size {len(balanced_data)} samples\")\n",
    "\n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1e695BotJZB"
   },
   "outputs": [],
   "source": [
    "## Baseline Datasets\n",
    "train_all, valid_all, test_all = generate_train_test_split(csv_filename)\n",
    "train_white, valid_white, test_white = generate_train_test_split(csv_filename, filter_query=\"P02RACE == '1: White or Caucasian'\")\n",
    "train_black, valid_black, test_black = generate_train_test_split(csv_filename, filter_query=\"P02RACE == '2: Black or African American'\")\n",
    "train_male, valid_male, test_male = generate_train_test_split(csv_filename, filter_query=\"P02SEX == '1: Male'\")\n",
    "train_female, valid_female, test_female = generate_train_test_split(csv_filename, filter_query=\"P02SEX == '2: Female'\")\n",
    "\n",
    "balanced_gender_train = balance_dataset(train_all, \"P02SEX == '1: Male'\", \"P02SEX == '2: Female'\")\n",
    "balanced_race_train = balance_dataset(train_all, \"P02RACE == '1: White or Caucasian'\", \"P02RACE == '2: Black or African American'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFxmkKsot_9J"
   },
   "source": [
    "## Baseline Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoQs24RmuUKX"
   },
   "outputs": [],
   "source": [
    "class BonyAnatomyJointSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, ids, num_classes, transforms=None, preprocessing=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.pids = ids\n",
    "        self.num_classes = num_classes\n",
    "        self.transforms = transforms\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def load_dicom(self, path):\n",
    "        dicom_img = pydicom.dcmread(path)\n",
    "        return dicom_img.pixel_array.astype(np.float32)\n",
    "\n",
    "    def load_nii(self, path):\n",
    "        nii_annot = nibabel.load(path)\n",
    "        nii_annot_data = nii_annot.get_fdata()\n",
    "\n",
    "        if len(nii_annot_data.shape) == 3 and nii_annot_data.shape[-1] > 1:\n",
    "            if nii_annot_data.shape[-1] == 2:\n",
    "                nii_annot_data = nii_annot_data[:, :, 1]\n",
    "            else:\n",
    "                nii_annot_data = nii_annot_data[:, :, nii_annot_data.shape[-1]//2]\n",
    "\n",
    "            nii_annot_data = np.expand_dims(nii_annot_data, axis=-1)\n",
    "\n",
    "\n",
    "        nii_annot_data = cv2.rotate(nii_annot_data, cv2.ROTATE_90_CLOCKWISE)\n",
    "        nii_annot_data = cv2.flip(nii_annot_data, 1)\n",
    "        return nii_annot_data\n",
    "\n",
    "    def get_file_path(self, filename):\n",
    "        return os.path.join(self.root_dir, filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = self.load_dicom(self.get_file_path(os.path.join(\"Images/\", str(self.pids[idx]) + \".dcm\")))\n",
    "        mask = self.load_nii(self.get_file_path(os.path.join(\"Annotations\", str(self.pids[idx]) + \".nii.gz\")))\n",
    "\n",
    "#         if len(np.unique(mask)) != self.num_classes:\n",
    "#             print(self.pids[idx])\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "        if self.preprocessing is not None:\n",
    "            transformed = self.preprocessing(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "\n",
    "        return image.type(torch.FloatTensor), mask.long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvQi5plPvfu3"
   },
   "outputs": [],
   "source": [
    "def train_model(train_loader, valid_loader, num_classes=9):\n",
    "    encoder = \"resnet18\"\n",
    "    encoder_weights = \"imagenet\"\n",
    "    activation = None\n",
    "\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = copy.deepcopy(smp.Unet(encoder_name=encoder, encoder_weights=encoder_weights, in_channels=1,\n",
    "                    classes=num_classes, activation=activation)).to(device)\n",
    "    model.encoder.requires_grad_ = False\n",
    "    model.decoder.requires_grad_ = False\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss.__name__=\" loss\"\n",
    "\n",
    "    multi_jaccard = MulticlassJaccardIndex(num_classes=num_classes, average=\"macro\").to(device)\n",
    "    multi_jaccard.__name__ = \"iou_score\"\n",
    "    metrics = [multi_jaccard]\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-04)\n",
    "\n",
    "    # create epoch runners\n",
    "    # it is a simple loop of iterating over dataloader`s samples\n",
    "    train_epoch = smp.utils.train.TrainEpoch(\n",
    "        model,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    valid_epoch = smp.utils.train.ValidEpoch(\n",
    "        model,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "        device=device,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    max_score = 0\n",
    "\n",
    "    for i in range(1, 51):\n",
    "\n",
    "        print('\\nEpoch: {}'.format(i))\n",
    "        train_logs = train_epoch.run(train_loader)\n",
    "        valid_logs = valid_epoch.run(valid_loader)\n",
    "\n",
    "        # do something (save model, change lr, etc.)\n",
    "        if max_score < valid_logs['iou_score']:\n",
    "            max_score = valid_logs['iou_score']\n",
    "            torch.save(model, './best_model.pth')\n",
    "            print('Model saved!')\n",
    "\n",
    "    # Return best model\n",
    "    model = torch.load('./best_model.pth')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNCMcoGoycz9"
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, class_labels):\n",
    "    model.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(10, 10), sharex=True, sharey=True)\n",
    "\n",
    "    x, y = next(iter(test_loader))\n",
    "    out = torch.softmax(model(x.to(device)), dim=1)\n",
    "    #out = out.detach().cpu().numpy()\n",
    "\n",
    "    for i, pred in enumerate(out):\n",
    "        uni_channels = torch.argmax(pred, dim=0).unique()\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        ax[i][0].imshow(x[i].squeeze(), cmap=\"gray\")\n",
    "        yi = y[i].squeeze()\n",
    "        ax[i][1].imshow(yi)\n",
    "\n",
    "        # Merge predicted masks into one image\n",
    "        mask = np.where(pred[uni_channels[0].item(),:,:] > 0.5, uni_channels[0].item(), 0)\n",
    "        for channel in uni_channels[1:]:\n",
    "            channel = channel.item()\n",
    "            channel_mask = np.where(pred[channel,:,:] > 0.5, channel, 0)\n",
    "            mask = mask | channel_mask\n",
    "        ax[i][2].imshow(mask)\n",
    "\n",
    "    ax[0][0].set_title(\"Image\")\n",
    "    ax[0][1].set_title(\"Ground Truth Mask\")\n",
    "    ax[0][2].set_title(\"Predicted Mask\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    multi_jaccard = MulticlassJaccardIndex(num_classes=num_classes, average=\"none\").cuda()\n",
    "    multi_jaccard.__name__ = \"iou\"\n",
    "    metrics = [multi_jaccard]\n",
    "\n",
    "    results = torch.zeros((1, num_classes))\n",
    "    for x, y in test_loader:\n",
    "        results += multi_jaccard(torch.softmax(model(x.cuda()), dim=1), y.cuda()).detach().cpu()\n",
    "\n",
    "    results = results/len(test_loader)\n",
    "\n",
    "    for i in range(0, len(class_labels)):\n",
    "        print(f\"{class_labels[i]} (Class {i}): {results[0][i]}\")\n",
    "\n",
    "    print(f\"Mean Testing IoU: {multi_jaccard(torch.softmax(model(x.cuda()), dim=1), y.cuda()).mean()}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGkOhTeavO0l"
   },
   "outputs": [],
   "source": [
    "augmentations = A.Compose([A.Resize(256, 256), ToTensorV2()])\n",
    "\n",
    "if BONY_ANATOMY == \"knee\":\n",
    "    num_classes = 9\n",
    "else:\n",
    "    num_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQExnRzVu0oh"
   },
   "outputs": [],
   "source": [
    "train_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, train_all.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "valid_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, valid_all.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "test_set = BonyAnatomyJointSegmentationDataset(imaging_data_root,test_all.id, num_classes,\n",
    "                                               transforms=augmentations)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xu2HCYrvLFm"
   },
   "outputs": [],
   "source": [
    "baseline_model = train_model(train_loader, valid_loader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNJqSHp8xM5w"
   },
   "outputs": [],
   "source": [
    "baseline_results = test_model(baseline_model, test_loader, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(baseline_model.state_dict(), \"checkpoints/baseline_knee.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WhVqAY60fmQ"
   },
   "source": [
    "## Gender Specific Baselines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-jqMABZ01F4"
   },
   "source": [
    "### Male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIHKRcCNy3Xg"
   },
   "outputs": [],
   "source": [
    "train_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, train_male.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "valid_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, valid_male.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "test_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, test_male.id, num_classes,\n",
    "                                               transforms=augmentations)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dggejIa0uuS"
   },
   "outputs": [],
   "source": [
    "baseline_male_model = train_model(train_loader, valid_loader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pz6FoGG90xJ0"
   },
   "outputs": [],
   "source": [
    "baseline_male_results = test_model(baseline_male_model, test_loader, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(baseline_male_model.state_dict(), \"checkpoints/baseline_male_knee.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qkz8__yN0xmE"
   },
   "source": [
    "### Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0vm6ojw0yXM"
   },
   "outputs": [],
   "source": [
    "train_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, train_female.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "valid_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, valid_female.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "test_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, test_female.id, num_classes,\n",
    "                                               transforms=augmentations)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2ixgt2b07tH"
   },
   "outputs": [],
   "source": [
    "baseline_female_model = train_model(train_loader, valid_loader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wk-QFFOw086q"
   },
   "outputs": [],
   "source": [
    "baseline_female_results = test_model(baseline_female_model, test_loader, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(baseline_female_model.state_dict(), \"checkpoints/baseline_female_knee.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcQL6vUu1C3z"
   },
   "source": [
    "## Race Specific Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V09lqyWf1DT9"
   },
   "source": [
    "### White/Caucasian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z887u24L1JMl"
   },
   "outputs": [],
   "source": [
    "train_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, train_white.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "valid_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, valid_white.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "test_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, test_white.id, num_classes,\n",
    "                                               transforms=augmentations)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsxG9HiF1QwF"
   },
   "outputs": [],
   "source": [
    "baseline_white_model = train_model(train_loader, valid_loader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgR51Z971S-8"
   },
   "outputs": [],
   "source": [
    "baseline_white_results = test_model(baseline_white_model, test_loader, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(baseline_white_model.state_dict(), \"checkpoints/baseline_white_knee.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOK8OSj11Vu3"
   },
   "source": [
    "### Black/African American (AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QSA1noA1Y3Z"
   },
   "outputs": [],
   "source": [
    "train_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, train_black.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "valid_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, valid_black.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "test_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, test_black.id, num_classes,\n",
    "                                               transforms=augmentations)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-I6xBZU1ig-"
   },
   "outputs": [],
   "source": [
    "baseline_black_model = train_model(train_loader, valid_loader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_gFVgU41iZB"
   },
   "outputs": [],
   "source": [
    "baseline_black_results = test_model(baseline_black_model, test_loader, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(baseline_black_model.state_dict(), \"checkpoints/baseline_black_aa_knee.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmQ4A8Qb1mM7"
   },
   "source": [
    "## Balanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQqJShEY1t61"
   },
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ga9bXQW1su1"
   },
   "outputs": [],
   "source": [
    "train_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, balanced_gender_train.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "valid_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, valid_all.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "test_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, test_all.id, num_classes,\n",
    "                                               transforms=augmentations)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQ7qvroD2G42"
   },
   "outputs": [],
   "source": [
    "balanced_gender_model = train_model(train_loader, valid_loader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUMufBhS2K5G"
   },
   "outputs": [],
   "source": [
    "balanced_gender_results = test_model(balanced_gender_model, test_loader, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(balanced_gender_model.state_dict(), \"checkpoints/balanced_gender_knee.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tB8QGCGp2NVL"
   },
   "source": [
    "### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XS4EVpNL2OUK"
   },
   "outputs": [],
   "source": [
    "train_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, balanced_race_train.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "valid_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, valid_all.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "test_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, test_all.id, num_classes,\n",
    "                                               transforms=augmentations)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0U_inJpS2Qn6"
   },
   "outputs": [],
   "source": [
    "balanced_race_model = train_model(train_loader, valid_loader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7hT7Ue62ScL"
   },
   "outputs": [],
   "source": [
    "balanced_race_results = test_model(balanced_race_model, test_loader, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(balanced_race_model.state_dict(), \"checkpoints/balanced_race_knee.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYzD99KV2Tlz"
   },
   "source": [
    "## Stratified Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqcwHfWU2fIj"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "class StratifiedSampler:\n",
    "    \"\"\"\n",
    "    Based on this Pytorch discussion board post\n",
    "    https://discuss.pytorch.org/t/how-to-enable-the-dataloader-to-sample-from-each-class-with-equal-probability/911/6\n",
    "    \"\"\"\n",
    "    def __init__(self, stratify_on, batch_size):\n",
    "        self.stratify_on = stratify_on\n",
    "        self.batch_size = batch_size\n",
    "        self.nsplits = int(len(stratify_on) / batch_size)\n",
    "\n",
    "    def gen_stratified_sample(self):\n",
    "        s = StratifiedKFold(n_splits = self.nsplits)\n",
    "\n",
    "        X = np.arange(0, len(self.stratify_on))\n",
    "        s.get_n_splits(X, self.stratify_on)\n",
    "        for train_idx, valid_idx in s.split(X, self.stratify_on):\n",
    "            yield valid_idx\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.gen_stratified_sample())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nsplits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfO8Zcmn2V6Y"
   },
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcXW9oX-2U3m"
   },
   "outputs": [],
   "source": [
    "train_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, train_all.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "valid_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, valid_all.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "test_set = BonyAnatomyJointSegmentationDataset(imaging_data_root,test_all.id, num_classes,\n",
    "                                               transforms=augmentations)\n",
    "\n",
    "train_loader = DataLoader(train_set, num_workers=2, batch_sampler=StratifiedSampler(train_all.P02SEX, batch_size=16))\n",
    "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iUzuAmK27ta"
   },
   "outputs": [],
   "source": [
    "stratified_gender_model = train_model(train_loader, valid_loader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkw-yitS2-iG"
   },
   "outputs": [],
   "source": [
    "stratified_gender_results = test_model(stratified_gender_model, test_loader, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(stratified_gender_model.state_dict(), \"checkpoints/stratified_gender_knee.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APvSw5Te3BJu"
   },
   "source": [
    "### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNhGw92A3CDK"
   },
   "outputs": [],
   "source": [
    "train_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, train_all.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "valid_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, valid_all.id, num_classes,\n",
    "                                                transforms=augmentations)\n",
    "\n",
    "test_set = BonyAnatomyJointSegmentationDataset(imaging_data_root,test_all.id, num_classes,\n",
    "                                               transforms=augmentations)\n",
    "\n",
    "train_loader = DataLoader(train_set, num_workers=2, batch_sampler=StratifiedSampler(train_all.P02RACE, batch_size=16))\n",
    "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0V-vHw8B3Eq_"
   },
   "outputs": [],
   "source": [
    "stratified_race_model = train_model(train_loader, valid_loader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnQhEyzv3H1h"
   },
   "outputs": [],
   "source": [
    "stratified_race_results = test_model(stratified_race_model, test_loader, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(stratified_race_model.state_dict(), \"checkpoints/stratified_race_knee.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wLJyTBiIUcn"
   },
   "source": [
    "## Evaluation Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XW7id1fHI3n2"
   },
   "outputs": [],
   "source": [
    "def eval_bias(model, dataset, metric, num_classes = 9):\n",
    "    results = torch.zeros((1, num_classes))\n",
    "    test_loader = generate_dataloader(dataset, num_classes)\n",
    "    skipped = 0\n",
    "    for x, y in test_loader:\n",
    "        if(len(y.unique()) > num_classes):\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        results += metric(torch.softmax(model(x.cuda()), dim=1), y.cuda()).detach().cpu()\n",
    "    \n",
    "    print(f\"Skipped {skipped} images due to class label mismatch...\")\n",
    "    return results/len(test_loader)\n",
    "\n",
    "def generate_dataloader(dataset, num_classes):\n",
    "    augmentations = A.Compose([A.Resize(256, 256), ToTensorV2()]) # A.OneOf([A.Emboss(), Canny()\n",
    "    test_set = BonyAnatomyJointSegmentationDataset(imaging_data_root, dataset.id, num_classes,\n",
    "                                                   transforms=augmentations)\n",
    "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9tiKIDKJBeR"
   },
   "outputs": [],
   "source": [
    "multi_jaccard = MulticlassJaccardIndex(num_classes=num_classes, average=\"none\").cuda()\n",
    "multi_jaccard.__name__ = \"iou\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtSiyDvEIjxK"
   },
   "outputs": [],
   "source": [
    "data_records = pd.read_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dViCGL1IWOQ"
   },
   "outputs": [],
   "source": [
    "male_group = data_records[data_records.P02SEX == \"1: Male\"].reset_index(drop=True)\n",
    "female_group = data_records[data_records.P02SEX == \"2: Female\"].reset_index(drop=True)\n",
    "white_caucasian_group = data_records[data_records.P02RACE == \"1: White or Caucasian\"].reset_index(drop=True)\n",
    "black_aa_group = data_records[data_records.P02RACE == \"2: Black or African American\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6-6QFQlIwQW"
   },
   "outputs": [],
   "source": [
    "gender_male_metrics_baseline = eval_bias(baseline_model, male_group, multi_jaccard)\n",
    "gender_female_metrics_baseline = eval_bias(baseline_model, female_group, multi_jaccard)\n",
    "race_white_metrics_baseline = eval_bias(baseline_model, white_caucasian_group, multi_jaccard)\n",
    "race_black_metrics_baseline = eval_bias(baseline_model, black_aa_group, multi_jaccard)\n",
    "\n",
    "base_dict = {\n",
    "    \"Gender: Male\": gender_male_metrics_baseline.numpy().squeeze(),\n",
    "    \"Gender: Female\": gender_female_metrics_baseline.numpy().squeeze(),\n",
    "    \"Race: White/Caucasian\": race_white_metrics_baseline.numpy().squeeze(),\n",
    "    \"Race: Black/AA\": race_black_metrics_baseline.numpy().squeeze(),\n",
    "}\n",
    "\n",
    "baseline_res = pd.DataFrame(base_dict).T\n",
    "baseline_res[\"Average\"] = baseline_res.mean(axis=1)\n",
    "baseline_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
    "baseline_res\n",
    "\n",
    "baseline_res.to_csv(\"results/baseline_knee.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0N3EhkmKFjv"
   },
   "outputs": [],
   "source": [
    "gender_male_metrics_balanced = eval_bias(balanced_gender_model, male_group, multi_jaccard)\n",
    "gender_female_metrics_balanced = eval_bias(balanced_gender_model, female_group, multi_jaccard)\n",
    "\n",
    "balanced_gender = {\n",
    "    \"Gender: Male\": gender_male_metrics_balanced.numpy().squeeze(),\n",
    "    \"Gender: Female\": gender_female_metrics_balanced.numpy().squeeze(),\n",
    "}\n",
    "balanced_gender_res = pd.DataFrame(balanced_gender).T\n",
    "balanced_gender_res[\"Average\"] =  balanced_gender_res.mean(axis=1)\n",
    "balanced_gender_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
    "\n",
    "balanced_gender_res\n",
    "\n",
    "balanced_gender_res.to_csv(\"results/balanced_gender_knee.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xm6YQ9RkKV5D"
   },
   "outputs": [],
   "source": [
    "race_white_metrics_balanced = eval_bias(balanced_race_model, white_caucasian_group, multi_jaccard)\n",
    "race_black_metrics_balanced = eval_bias(balanced_race_model, black_aa_group, multi_jaccard)\n",
    "\n",
    "balanced_race = {\n",
    "     \"Race: White/Caucasian\": race_white_metrics_balanced.numpy().squeeze(),\n",
    "    \"Race: Black/AA\": race_black_metrics_balanced.numpy().squeeze(),\n",
    "}\n",
    "balanced_race_res = pd.DataFrame(balanced_race).T\n",
    "balanced_race_res[\"Average\"] = balanced_race_res.mean(axis=1)\n",
    "balanced_race_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
    "\n",
    "balanced_race_res\n",
    "balanced_race_res.to_csv(\"results/balanced_race_knee.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UY0j5j6zK_yz"
   },
   "outputs": [],
   "source": [
    "gender_male_metrics_stratified = eval_bias(stratified_gender_model, male_group, multi_jaccard)\n",
    "gender_female_metrics_stratified = eval_bias(stratified_gender_model, female_group, multi_jaccard)\n",
    "\n",
    "strat_gender = {\n",
    "    \"Gender: Male\": gender_male_metrics_stratified.numpy().squeeze(),\n",
    "    \"Gender: Female\": gender_female_metrics_stratified.numpy().squeeze(),\n",
    "}\n",
    "strat_gender_res = pd.DataFrame(strat_gender).T\n",
    "strat_gender_res[\"Average\"] = strat_gender_res.mean(axis=1)\n",
    "strat_gender_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
    "strat_gender_res\n",
    "\n",
    "strat_gender_res.to_csv(\"results/strat_gender_knee.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xd-7qEYTK3EI"
   },
   "outputs": [],
   "source": [
    "race_white_metrics_stratified = eval_bias(stratified_race_model, white_caucasian_group, multi_jaccard)\n",
    "race_black_metrics_stratified= eval_bias(stratified_race_model, black_aa_group, multi_jaccard)\n",
    "\n",
    "strat_race = {\n",
    "    \"Race: White/Caucasian\": race_white_metrics_stratified.numpy().squeeze(),\n",
    "    \"Race: Black/AA\": race_black_metrics_stratified.numpy().squeeze(),\n",
    "}\n",
    "strat_race_res = pd.DataFrame(strat_race).T\n",
    "strat_race_res[\"Average\"] = strat_race_res.mean(axis=1)\n",
    "strat_race_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
    "strat_race_res\n",
    "\n",
    "strat_race_res.to_csv(\"results/strat_race_knee.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6X_5WxHmLKTb"
   },
   "outputs": [],
   "source": [
    "gender_male_metrics = eval_bias(baseline_male_model, male_group, multi_jaccard)\n",
    "\n",
    "gender_male = {\n",
    "    \"Gender: Male\": gender_male_metrics.numpy().squeeze(),\n",
    "}\n",
    "gender_male = pd.DataFrame(gender_male).T\n",
    "gender_male[\"Average\"] = gender_male.mean(axis=1)\n",
    "gender_male.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
    "gender_male\n",
    "\n",
    "gender_male.to_csv(\"results/gender_spec_male_knee.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wxnEBqRLUyn"
   },
   "outputs": [],
   "source": [
    "gender_female_metrics = eval_bias(baseline_female_model, female_group, multi_jaccard)\n",
    "\n",
    "gender_female = {\n",
    "    \"Gender: Female\": gender_female_metrics.numpy().squeeze(),\n",
    "}\n",
    "gender_female = pd.DataFrame(gender_female).T\n",
    "gender_female[\"Average\"] = gender_female.mean(axis=1)\n",
    "gender_female.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
    "gender_female\n",
    "\n",
    "gender_female.to_csv(\"results/gender_spec_female_knee.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lf31YIEZLcEl"
   },
   "outputs": [],
   "source": [
    "race_white_metrics = eval_bias(baseline_white_model, white_caucasian_group, multi_jaccard)\n",
    "\n",
    "race_white = {\n",
    "    \"Race: White/Caucasian\": race_white_metrics.numpy().squeeze(),\n",
    "}\n",
    "race_white = pd.DataFrame(race_white).T\n",
    "race_white[\"Average\"] = race_white.mean(axis=1)\n",
    "race_white.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
    "race_white\n",
    "\n",
    "race_white.to_csv(\"results/race_spec_white_knee.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNeHkyERLpNv"
   },
   "outputs": [],
   "source": [
    "race_black_metrics = eval_bias(baseline_black_model, black_aa_group, multi_jaccard)\n",
    "\n",
    "race_black = {\n",
    "    \"Race: Black/AA\": race_black_metrics.numpy().squeeze(),\n",
    "}\n",
    "race_black = pd.DataFrame(race_black).T\n",
    "race_black[\"Average\"] = race_black.mean(axis=1)\n",
    "race_black.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
    "race_black\n",
    "race_black.to_csv(\"results/race_spec_black_knee.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_DvIH88L5WB"
   },
   "outputs": [],
   "source": [
    "def calc_fairness_score(data):\n",
    "    sd = []\n",
    "    ser = []\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        sd.append(row[1:].std())\n",
    "        min_group = row[1:].min()\n",
    "        max_group = row[1:].max()\n",
    "        ser.append((1 - min_group)/(1 - max_group))\n",
    "\n",
    "    return sd, ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozSgh74AMA20"
   },
   "outputs": [],
   "source": [
    "fairness_gender = {\n",
    "    \"Model\": [\"Baseline\", \"Balanced\", \"Stratified\", \"Group-Specific\"],\n",
    "    \"Male\": [baseline_res.loc[\"Gender: Male\", \"Average\"],\n",
    "             balanced_gender_res.loc[\"Gender: Male\", \"Average\"],\n",
    "             strat_gender_res.loc[\"Gender: Male\", \"Average\"],\n",
    "             gender_male.loc[\"Gender: Male\", \"Average\"]],\n",
    "    \"Female\":[baseline_res.loc[\"Gender: Female\", \"Average\"],\n",
    "             balanced_gender_res.loc[\"Gender: Female\", \"Average\"],\n",
    "             strat_gender_res.loc[\"Gender: Female\", \"Average\"],\n",
    "             gender_female.loc[\"Gender: Female\", \"Average\"]]\n",
    "}\n",
    "\n",
    "fairness_df_gender = pd.DataFrame(fairness_gender)\n",
    "sd, ser = calc_fairness_score(fairness_df_gender)\n",
    "fairness_df_gender[\"SD\"] = sd\n",
    "fairness_df_gender[\"SER\"] = ser\n",
    "\n",
    "fairness_df_gender\n",
    "\n",
    "fairness_df_gender.to_csv(\"results/fairness_gender_knee.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFAyOnhAMEN6"
   },
   "outputs": [],
   "source": [
    "fairness_race = {\n",
    "    \"Model\": [\"Baseline\", \"Balanced\", \"Stratified\", \"Group-Specific\"],\n",
    "    \"White/Caucasian\": [baseline_res.loc[\"Race: White/Caucasian\", \"Average\"],\n",
    "             balanced_race_res.loc[\"Race: White/Caucasian\", \"Average\"],\n",
    "             strat_race_res.loc[\"Race: White/Caucasian\", \"Average\"],\n",
    "             race_white.loc[\"Race: White/Caucasian\", \"Average\"]],\n",
    "    \"Black/African American\":[baseline_res.loc[\"Race: Black/AA\", \"Average\"],\n",
    "             balanced_race_res.loc[\"Race: Black/AA\", \"Average\"],\n",
    "             strat_race_res.loc[\"Race: Black/AA\", \"Average\"],\n",
    "             race_black.loc[\"Race: Black/AA\", \"Average\"]]\n",
    "}\n",
    "\n",
    "fairness_df_race = pd.DataFrame(fairness_race)\n",
    "sd, ser = calc_fairness_score(fairness_df_race)\n",
    "fairness_df_race[\"SD\"] = sd\n",
    "fairness_df_race[\"SER\"] = ser\n",
    "\n",
    "fairness_df_race\n",
    "fairness_df_gender.to_csv(\"results/fairness_race_knee.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANz0PcXYNmsB"
   },
   "outputs": [],
   "source": [
    "5e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP7vlrtZd8GhpPjeRMfm7wr",
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch113_p310_gpu_v1]",
   "language": "python",
   "name": "conda-env-pytorch113_p310_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
